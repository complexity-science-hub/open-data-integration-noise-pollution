{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas_profiling\n",
    "%pylab inline\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from analysis.settings.settings import DevelopmentConfig\n",
    "c = DevelopmentConfig\n",
    "\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "from shapely.geometry.polygon import Polygon\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open data integration\n",
    "Cleaning shape files delivered as open data from http://www.laerminfo.at/laermkarten/methoden/inspire.html\n",
    "\n",
    "> an accompanying blog post can be found at: https://georgheiler.com/2019/02/08/noise-pollution-data-cleanup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_dirs(directory_in_str, glob):\n",
    "    pathlist = Path(directory_in_str).glob(glob)\n",
    "    for path in pathlist:\n",
    "        yield str(path)\n",
    "        \n",
    "def parse_attributes_from_path(path):\n",
    "    file_name = path.split('/')[-1]\n",
    "    elements = file_name.split('_')\n",
    "    result = {}\n",
    "    result['year'] = elements[0]\n",
    "    result['kind'] = elements[1]\n",
    "    result['timing'] = elements[2]\n",
    "    result['state'] = elements[-1].split('.')[0]\n",
    "    return result\n",
    "\n",
    "def add_columns_to_df(df, items):\n",
    "    for key, value in items.items():\n",
    "        df[key] = value\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = iter_dirs(c.BASE_PATH, '**/*.shp')\n",
    "#items = 2\n",
    "#print(f\"WARN UNSET THIS FOR PRODUCTION. currently only {items} are processed\")\n",
    "#paths = itertools.islice(paths, items)\n",
    "tmp_appended_data = []\n",
    "for path in tqdm(paths):\n",
    "    attributes_from_filenname = parse_attributes_from_path(path)\n",
    "    df = gp.read_file(path)\n",
    "    df = add_columns_to_df(df, attributes_from_filenname)\n",
    "    tmp_appended_data.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(tmp_appended_data, axis=0)\n",
    "df = df.reset_index(drop=True)\n",
    "display(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to all multipolygon shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def convert_polygon_to_multipolygon(raw_geometry):\n",
    "    if(isinstance(raw_geometry, shapely.geometry.polygon.Polygon)):\n",
    "        return MultiPolygon([raw_geometry])\n",
    "    else:\n",
    "        # we currently only have MULTIPOLYGON and POLYGON so plain else is good enough\n",
    "        return raw_geometry\n",
    "            \n",
    "df.geometry = df.geometry.apply(convert_polygon_to_multipolygon)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write output files\n",
    "Geopackage is a nice standard and a bit more structured than CSV. But CSV is more versatile i.e. as no spatial index and type information are present polygon and multipolygons can be put into the same column. That's why we needed to convert geometries in the step above.\n",
    "\n",
    "The cooridnate system is `EPSG:31287`, http://spatialreference.org/ref/epsg/mgi-austria-lambert-2/ it can be stored in the geodatabase for easier downstream processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.crs = {'init' :'epsg:31287'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(c.BASE_PATH + 'noise_pollution.gzip.csv', index=False, compression='gzip')\n",
    "# is a bit smaller in file size and has no spatial index / less optimal\n",
    "\n",
    "df.to_file(c.BASE_PATH + 'noise_pollution.gpkg', driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
